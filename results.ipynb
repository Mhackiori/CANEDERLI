{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.models import *\n",
    "from utils.params import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "setSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(vehicle, paths):\n",
    "    datasetPath = f'./dataset/{vehicle}_multi.csv'\n",
    "\n",
    "    df = pd.read_csv(datasetPath)\n",
    "    df = df.rename(columns={'Flag': 'Class'})\n",
    "\n",
    "    features = df.drop(['Class'], axis=1).values\n",
    "    labels = df['Class'].values\n",
    "\n",
    "    features, labels = RandomUnderSampler(random_state=seed).fit_resample(features, labels)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "    # Create DataLoader for training set\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Create DataLoader for test set\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    ### FCN ###\n",
    "\n",
    "    # Initialize the FCN model\n",
    "    input_size_fcn = len(df.columns) - 1\n",
    "    hidden_size_fcn = 64\n",
    "    output_size_fcn = 4\n",
    "    model_fcn = FCNMultiClass(input_size_fcn, hidden_size_fcn, output_size_fcn).to(device)\n",
    "\n",
    "    # Initialize the optimizer for the FCN model\n",
    "    optimizer_fcn = optim.Adam(model_fcn.parameters(), lr=0.001)\n",
    "\n",
    "    fcnPath = paths[0]\n",
    "\n",
    "    model_fcn.load_state_dict(torch.load(fcnPath))\n",
    "\n",
    "    # Evaluate the FCN model\n",
    "    accuracy_fcn, f1_fcn = evaluate_multi_class_model(model_fcn, test_dataloader, device)\n",
    "    print(f'[üëë FCN {vehicle}] Accuracy: {accuracy_fcn:.3f}, F1: {f1_fcn:.3f}')\n",
    "\n",
    "    ### CNN ###\n",
    "\n",
    "    # Initialize the CNN model\n",
    "    input_size_cnn = len(df.columns) - 1\n",
    "    output_size_cnn = 4\n",
    "    model_cnn = CNNMultiClass(input_size_cnn, output_size_cnn).to(device)\n",
    "\n",
    "    # Initialize the optimizer for the CNN model\n",
    "    optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "    cnnPath = paths[1]\n",
    "\n",
    "    model_cnn.load_state_dict(torch.load(cnnPath))\n",
    "\n",
    "    # Evaluate the CNN model\n",
    "    accuracy_cnn, f1_cnn = evaluate_multi_class_model(model_cnn, test_dataloader, device)\n",
    "    print(f'[üëë CNN {vehicle}] Accuracy: {accuracy_cnn:.3f}, F1: {f1_cnn:.3f}')\n",
    "\n",
    "    ### LSTM ###\n",
    "    input_size_lstm = len(df.columns) - 1  # Adjust this based on your data\n",
    "    hidden_size_lstm = 64\n",
    "    output_size_lstm = 4\n",
    "    model_lstm = LSTMMultiClass(input_size_lstm, hidden_size_lstm, output_size_lstm).to(device)\n",
    "    \n",
    "    optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "    lstmPath = paths[2]\n",
    "\n",
    "    model_lstm.load_state_dict(torch.load(lstmPath))\n",
    "\n",
    "    # Evaluate the FCN model\n",
    "    accuracy_lstm, f1_lstm = evaluate_multi_class_model(model_lstm, test_dataloader, device)\n",
    "    print(f'[üëë LSTM {vehicle}] Accuracy: {accuracy_lstm:.3f}, F1: {f1_lstm:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/FCN_multi.pth', f'./models/{vehicle}/CNN_multi.pth', f'./models/{vehicle}/LSTM_multi.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öîÔ∏è Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsFolder = './results/'\n",
    "transferabilityFolder = os.path.join(resultsFolder, 'transferability')\n",
    "\n",
    "sonata_t = os.path.join(transferabilityFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(transferabilityFolder, 'soul.csv')\n",
    "spark_t = os.path.join(transferabilityFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-attacks')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-attacks/FCN.pth', f'./models/{vehicle}/adversarial-training/all-attacks/CNN.pth', f'./models/{vehicle}/adversarial-training/all-attacks/LSTM.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-models')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-models/FCN.pth', f'./models/{vehicle}/adversarial-training/all-models/CNN.pth', f'./models/{vehicle}/adversarial-training/all-models/LSTM.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-vehicles')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-vehicles/FCN.pth', f'./models/{vehicle}/adversarial-training/all-vehicles/CNN.pth', f'./models/{vehicle}/adversarial-training/all-vehicles/LSTM.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∏Ô∏è Online Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-attacks')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata_online.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul_online.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark_online.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-attacks/FCN_online.pth', f'./models/{vehicle}/adversarial-training/all-attacks/CNN_online.pth', f'./models/{vehicle}/adversarial-training/all-attacks/LSTM_online.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-models')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata_online.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul_online.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark_online.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-models/FCN_online.pth', f'./models/{vehicle}/adversarial-training/all-models/CNN_online.pth', f'./models/{vehicle}/adversarial-training/all-models/LSTM_online.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
