{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.models import *\n",
    "from utils.params import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "setSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ðŸ‘‘ FCN sonata] Accuracy: 1.000, F1: 1.000\n",
      "[ðŸ‘‘ CNN sonata] Accuracy: 0.999, F1: 0.999\n",
      "[ðŸ‘‘ LSTM sonata] Accuracy: 1.000, F1: 1.000\n",
      "\n",
      "[ðŸ‘‘ FCN soul] Accuracy: 0.995, F1: 0.995\n",
      "[ðŸ‘‘ CNN soul] Accuracy: 0.993, F1: 0.993\n",
      "[ðŸ‘‘ LSTM soul] Accuracy: 0.995, F1: 0.995\n",
      "\n",
      "[ðŸ‘‘ FCN spark] Accuracy: 0.997, F1: 0.997\n",
      "[ðŸ‘‘ CNN spark] Accuracy: 0.997, F1: 0.997\n",
      "[ðŸ‘‘ LSTM spark] Accuracy: 0.999, F1: 0.999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiclass\n",
    "for vehicle in vehicles:\n",
    "    datasetPath = f'./dataset/{vehicle}_multi.csv'\n",
    "\n",
    "    df = pd.read_csv(datasetPath)\n",
    "    df = df.rename(columns={'Flag': 'Class'})\n",
    "\n",
    "    features = df.drop(['Class'], axis=1).values\n",
    "    labels = df['Class'].values\n",
    "\n",
    "    features, labels = RandomUnderSampler(random_state=seed).fit_resample(features, labels)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "    # Create DataLoader for training set\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Create DataLoader for test set\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    ### FCN ###\n",
    "\n",
    "    # Initialize the FCN model\n",
    "    input_size_fcn = len(df.columns) - 1\n",
    "    hidden_size_fcn = 64\n",
    "    output_size_fcn = 4\n",
    "    model_fcn = FCNMultiClass(input_size_fcn, hidden_size_fcn, output_size_fcn).to(device)\n",
    "\n",
    "    # Initialize the optimizer for the FCN model\n",
    "    optimizer_fcn = optim.Adam(model_fcn.parameters(), lr=0.001)\n",
    "\n",
    "    fcnPath = f'./models/{vehicle}/FCN_multi.pth'\n",
    "\n",
    "    model_fcn.load_state_dict(torch.load(fcnPath))\n",
    "\n",
    "    # Evaluate the FCN model\n",
    "    accuracy_fcn, f1_fcn = evaluate_multi_class_model(model_fcn, test_dataloader, device)\n",
    "    print(f'[ðŸ‘‘ FCN {vehicle}] Accuracy: {accuracy_fcn:.3f}, F1: {f1_fcn:.3f}')\n",
    "\n",
    "    ### CNN ###\n",
    "\n",
    "    # Initialize the CNN model\n",
    "    input_size_cnn = len(df.columns) - 1\n",
    "    output_size_cnn = 4\n",
    "    model_cnn = CNNMultiClass(input_size_cnn, output_size_cnn).to(device)\n",
    "\n",
    "    # Initialize the optimizer for the CNN model\n",
    "    optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "    cnnPath = f'./models/{vehicle}/CNN_multi.pth'\n",
    "\n",
    "    model_cnn.load_state_dict(torch.load(cnnPath))\n",
    "\n",
    "    # Evaluate the CNN model\n",
    "    accuracy_cnn, f1_cnn = evaluate_multi_class_model(model_cnn, test_dataloader, device)\n",
    "    print(f'[ðŸ‘‘ CNN {vehicle}] Accuracy: {accuracy_cnn:.3f}, F1: {f1_cnn:.3f}')\n",
    "\n",
    "    ### LSTM ###\n",
    "    input_size_lstm = len(df.columns) - 1  # Adjust this based on your data\n",
    "    hidden_size_lstm = 64\n",
    "    output_size_lstm = 4\n",
    "    model_lstm = LSTMMultiClass(input_size_lstm, hidden_size_lstm, output_size_lstm).to(device)\n",
    "    \n",
    "    optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "    lstmPath = f'./models/{vehicle}/LSTM_multi.pth'\n",
    "\n",
    "    model_lstm.load_state_dict(torch.load(lstmPath))\n",
    "\n",
    "    # Evaluate the FCN model\n",
    "    accuracy_lstm, f1_lstm = evaluate_multi_class_model(model_lstm, test_dataloader, device)\n",
    "    print(f'[ðŸ‘‘ LSTM {vehicle}] Accuracy: {accuracy_lstm:.3f}, F1: {f1_lstm:.3f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsFolder = './results/'\n",
    "transferabilityFolder = os.path.join(resultsFolder, 'transferability')\n",
    "\n",
    "sonata_t = os.path.join(transferabilityFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(transferabilityFolder, 'soul.csv')\n",
    "spark_t = os.path.join(transferabilityFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODELS F1]\t[WHITE]\t[BLACK]\n",
      "[FCN]\t\t0.257\t0.246\n",
      "[CNN]\t\t0.462\t0.374\n",
      "[LSTM]\t\t0.254\t0.283\n",
      "\n",
      "[ATTACKS F1]\t[WHITE]\t[BLACK]\n",
      "[BIM WB]\t0.362\t0.356\n",
      "[FGSM WB]\t0.295\t0.251\n",
      "[PGD WB]\t0.285\t0.265\n",
      "[RFGSM WB]\t0.356\t0.333\n"
     ]
    }
   ],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-attacks')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODELS F1]\t[WHITE]\t[BLACK]\n",
      "[FCN]\t\t0.956\t0.679\n",
      "[CNN]\t\t0.997\t0.692\n",
      "[LSTM]\t\t0.963\t0.661\n",
      "\n",
      "[ATTACKS F1]\t[WHITE]\t[BLACK]\n",
      "[BIM WB]\t0.971\t0.689\n",
      "[FGSM WB]\t0.972\t0.646\n",
      "[PGD WB]\t0.974\t0.686\n",
      "[RFGSM WB]\t0.972\t0.689\n"
     ]
    }
   ],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-models')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODELS F1]\t[WHITE]\t[BLACK]\n",
      "[FCN]\t\t0.682\t0.741\n",
      "[CNN]\t\t0.450\t0.513\n",
      "[LSTM]\t\t0.738\t0.758\n",
      "\n",
      "[ATTACKS F1]\t[WHITE]\t[BLACK]\n",
      "[BIM WB]\t0.687\t0.704\n",
      "[FGSM WB]\t0.494\t0.638\n",
      "[PGD WB]\t0.627\t0.647\n",
      "[RFGSM WB]\t0.685\t0.693\n"
     ]
    }
   ],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-vehicles')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODELS F1]\t[WHITE]\t[BLACK]\n",
      "[FCN]\t\t0.678\t0.726\n",
      "[CNN]\t\t0.400\t0.447\n",
      "[LSTM]\t\t0.643\t0.687\n",
      "\n",
      "[ATTACKS F1]\t[WHITE]\t[BLACK]\n",
      "[BIM WB]\t0.633\t0.659\n",
      "[FGSM WB]\t0.461\t0.583\n",
      "[PGD WB]\t0.571\t0.593\n",
      "[RFGSM WB]\t0.631\t0.645\n"
     ]
    }
   ],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-attacks')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata_online.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul_online.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark_online.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODELS F1]\t[WHITE]\t[BLACK]\n",
      "[FCN]\t\t0.936\t0.671\n",
      "[CNN]\t\t0.880\t0.621\n",
      "[LSTM]\t\t0.941\t0.673\n",
      "\n",
      "[ATTACKS F1]\t[WHITE]\t[BLACK]\n",
      "[BIM WB]\t0.950\t0.673\n",
      "[FGSM WB]\t0.830\t0.605\n",
      "[PGD WB]\t0.946\t0.672\n",
      "[RFGSM WB]\t0.950\t0.672\n"
     ]
    }
   ],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-models')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata_online.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul_online.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark_online.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODELS F1]\t[WHITE]\t[BLACK]\n",
      "[FCN]\t\t0.274\t0.244\n",
      "[CNN]\t\t0.461\t0.373\n",
      "[LSTM]\t\t0.925\t0.659\n",
      "\n",
      "[ATTACKS F1]\t[WHITE]\t[BLACK]\n",
      "[BIM WB]\t0.602\t0.465\n",
      "[FGSM WB]\t0.468\t0.365\n",
      "[PGD WB]\t0.543\t0.413\n",
      "[RFGSM WB]\t0.600\t0.458\n"
     ]
    }
   ],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name} WB]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
