{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.helpers import *\n",
    "from utils.models import *\n",
    "from utils.params import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "setSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(vehicle, paths):\n",
    "    datasetPath = f'./dataset/{vehicle}_multi.csv'\n",
    "\n",
    "    df = pd.read_csv(datasetPath)\n",
    "    df = df.rename(columns={'Flag': 'Class'})\n",
    "\n",
    "    features = df.drop(['Class'], axis=1).values\n",
    "    labels = df['Class'].values\n",
    "\n",
    "    features, labels = RandomUnderSampler(random_state=seed).fit_resample(features, labels)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "    # Create DataLoader for training set\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    # Create DataLoader for test set\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    ### FCN ###\n",
    "\n",
    "    # Initialize the FCN model\n",
    "    input_size_fcn = len(df.columns) - 1\n",
    "    hidden_size_fcn = 64\n",
    "    output_size_fcn = 4\n",
    "    model_fcn = FCNMultiClass(input_size_fcn, hidden_size_fcn, output_size_fcn).to(device)\n",
    "\n",
    "    # Initialize the optimizer for the FCN model\n",
    "    optimizer_fcn = optim.Adam(model_fcn.parameters(), lr=0.001)\n",
    "\n",
    "    fcnPath = paths[0]\n",
    "\n",
    "    model_fcn.load_state_dict(torch.load(fcnPath))\n",
    "\n",
    "    # Evaluate the FCN model\n",
    "    accuracy_fcn, f1_fcn = evaluate_multi_class_model(model_fcn, test_dataloader, device)\n",
    "    print(f'[üëë FCN {vehicle}] Accuracy: {accuracy_fcn:.3f}, F1: {f1_fcn:.3f}')\n",
    "\n",
    "    ### CNN ###\n",
    "\n",
    "    # Initialize the CNN model\n",
    "    input_size_cnn = len(df.columns) - 1\n",
    "    output_size_cnn = 4\n",
    "    model_cnn = CNNMultiClass(input_size_cnn, output_size_cnn).to(device)\n",
    "\n",
    "    # Initialize the optimizer for the CNN model\n",
    "    optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "\n",
    "    cnnPath = paths[1]\n",
    "\n",
    "    model_cnn.load_state_dict(torch.load(cnnPath))\n",
    "\n",
    "    # Evaluate the CNN model\n",
    "    accuracy_cnn, f1_cnn = evaluate_multi_class_model(model_cnn, test_dataloader, device)\n",
    "    print(f'[üëë CNN {vehicle}] Accuracy: {accuracy_cnn:.3f}, F1: {f1_cnn:.3f}')\n",
    "\n",
    "    ### LSTM ###\n",
    "    input_size_lstm = len(df.columns) - 1  # Adjust this based on your data\n",
    "    hidden_size_lstm = 64\n",
    "    output_size_lstm = 4\n",
    "    model_lstm = LSTMMultiClass(input_size_lstm, hidden_size_lstm, output_size_lstm).to(device)\n",
    "    \n",
    "    optimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "    lstmPath = paths[2]\n",
    "\n",
    "    model_lstm.load_state_dict(torch.load(lstmPath))\n",
    "\n",
    "    # Evaluate the FCN model\n",
    "    accuracy_lstm, f1_lstm = evaluate_multi_class_model(model_lstm, test_dataloader, device)\n",
    "    print(f'[üëë LSTM {vehicle}] Accuracy: {accuracy_lstm:.3f}, F1: {f1_lstm:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/FCN_multi.pth', f'./models/{vehicle}/CNN_multi.pth', f'./models/{vehicle}/LSTM_multi.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öîÔ∏è Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsFolder = './results/'\n",
    "transferabilityFolder = os.path.join(resultsFolder, 'transferability')\n",
    "\n",
    "sonata_t = os.path.join(transferabilityFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(transferabilityFolder, 'soul.csv')\n",
    "spark_t = os.path.join(transferabilityFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attacks_evaluation(df, bw='white'):\n",
    "    \"\"\"\n",
    "    Takes in input a list of DataFrames and white/gray/black-box type evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    models_acc = []\n",
    "    models_f1 = []\n",
    "\n",
    "    attacks_acc = []\n",
    "    attacks_f1 = []\n",
    "\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "\n",
    "    if bw == 'white':\n",
    "        sub = df[df['Target_Vehicle'] == df['Source_Vehicle']]\n",
    "        sub = sub[sub['Target_Model'] == sub['Source_Model']]\n",
    "    elif bw == 'gray':\n",
    "        sub = df[(df['Target_Vehicle'] != df['Source_Vehicle']) & (df['Target_Model'] == df['Source_Model']) |\n",
    "                 (df['Target_Vehicle'] == df['Source_Vehicle']) & (df['Target_Model'] != df['Source_Model'])]\n",
    "        # sub = df[df['Target_Vehicle'] != df['Source_Vehicle']]\n",
    "        # sub = sub[sub['Target_Model'] == sub['Source_Model']]\n",
    "    elif bw == 'black':\n",
    "        sub = df[df['Target_Vehicle'] != df['Source_Vehicle']]\n",
    "        sub = sub[sub['Target_Model'] != sub['Source_Model']]    \n",
    "    else:\n",
    "        raise ValueError('bw must be either white or black')\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model_df = sub[sub['Target_Model'] == model_name]\n",
    "\n",
    "        models_acc.append(model_df['Accuracy'].mean()) \n",
    "        models_f1.append(model_df['F1'].mean())\n",
    "\n",
    "    for attack_name in attack_names:\n",
    "        attack_df = sub[sub['Attack'] == attack_name]\n",
    "\n",
    "        attacks_acc.append(attack_df['Accuracy'].mean())\n",
    "        attacks_f1.append(attack_df['F1'].mean())\n",
    "        \n",
    "    accuracy.append(sub['Accuracy'].mean())\n",
    "    f1.append(sub['F1'].mean())\n",
    "    \n",
    "\n",
    "    return models_acc, models_f1, attacks_acc, attacks_f1, np.mean(accuracy), np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation(df, bw='gray')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attacks_evaluation_eps(df, eps):\n",
    "    \"\"\"\n",
    "    Takes in input a list of DataFrames and white/gray/black-box type evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    models_acc = []\n",
    "    models_f1 = []\n",
    "\n",
    "    attacks_acc = []\n",
    "    attacks_f1 = []\n",
    "\n",
    "    accuracy = []\n",
    "    f1 = []\n",
    "\n",
    "    if eps == 0.1:\n",
    "        sub = df[df['Epsilon'] == 0.1]\n",
    "    elif eps == 0.2:\n",
    "        sub = df[df['Epsilon'] == 0.2]\n",
    "    elif eps == 0.3:\n",
    "        sub = df[df['Epsilon'] == 0.3]\n",
    "    else:\n",
    "        raise ValueError('eps must be either 0.1, 0.2 or 0.3')\n",
    "\n",
    "    for model_name in model_names:\n",
    "        model_df = sub[sub['Target_Model'] == model_name]\n",
    "\n",
    "        models_acc.append(model_df['Accuracy'].mean()) \n",
    "        models_f1.append(model_df['F1'].mean())\n",
    "\n",
    "    for attack_name in attack_names:\n",
    "        attack_df = sub[sub['Attack'] == attack_name]\n",
    "\n",
    "        attacks_acc.append(attack_df['Accuracy'].mean())\n",
    "        attacks_f1.append(attack_df['F1'].mean())\n",
    "        \n",
    "    accuracy.append(sub['Accuracy'].mean())\n",
    "    f1.append(sub['F1'].mean())\n",
    "    \n",
    "\n",
    "    return models_acc, models_f1, attacks_acc, attacks_f1, np.mean(accuracy), np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation_eps(df, eps=0.1)\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation_eps(df, eps=0.2)\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation_eps(df, eps=0.3)\n",
    "\n",
    "print('[MODELS F1]\\t[0.1]\\t[0.2]\\t[0.3]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[0.1]\\t[0.2]\\t[0.3]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = [0.367, 0.337, 0.329, 0.348]\n",
    "e2 = [0.367, 0.276, 0.272, 0.348]\n",
    "e3 = [0.367, 0.199, 0.237, 0.348]\n",
    "\n",
    "num_groups = 4\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "x = np.arange(num_groups)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(5, 3.5)\n",
    "\n",
    "for i in range(num_groups):\n",
    "    ax.bar(x[i] - bar_width, e1[i], bar_width, capsize=5, label='WebServer', color='tab:blue', zorder=3)\n",
    "    ax.bar(x[i], e2[i], bar_width, capsize=5, label='Routing', color='tab:orange', zorder=3)\n",
    "    ax.bar(x[i] + bar_width, e3[i], bar_width, capsize=5, label='Intradomain', color='tab:green', zorder=3)\n",
    "\n",
    "ax.set_xlabel('Attacks')\n",
    "ax.set_ylabel('Average F1 Score')\n",
    "ax.set_ylim([0, 0.5])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['BIM', 'FGSM', 'PGD', 'RFGSM'])\n",
    "ax.legend(['Œµ = 0.1', 'Œµ = 0.2', 'Œµ = 0.3'])\n",
    "\n",
    "plt.tight_layout()\n",
    "ax.grid(axis='y', linestyle='-', zorder=0)\n",
    "plt.savefig('./figures/Attacks.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-attacks')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation(df, bw='gray')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-attacks/FCN.pth', f'./models/{vehicle}/adversarial-training/all-attacks/CNN.pth', f'./models/{vehicle}/adversarial-training/all-attacks/LSTM.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-models')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation(df, bw='gray')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-models/FCN.pth', f'./models/{vehicle}/adversarial-training/all-models/CNN.pth', f'./models/{vehicle}/adversarial-training/all-models/LSTM.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-vehicles')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation(df, bw='gray')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-vehicles/FCN.pth', f'./models/{vehicle}/adversarial-training/all-vehicles/CNN.pth', f'./models/{vehicle}/adversarial-training/all-vehicles/LSTM.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∏Ô∏è Online Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-attacks')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata_online.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul_online.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark_online.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation(df, bw='gray')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-attacks/FCN_online.pth', f'./models/{vehicle}/adversarial-training/all-attacks/CNN_online.pth', f'./models/{vehicle}/adversarial-training/all-attacks/LSTM_online.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advFolder = os.path.join(resultsFolder, 'adversarial-training/all-models')\n",
    "\n",
    "sonata_t = os.path.join(advFolder, 'sonata_online.csv')\n",
    "soul_t = os.path.join(advFolder, 'soul_online.csv')\n",
    "spark_t = os.path.join(advFolder, 'spark_online.csv')\n",
    "\n",
    "sonata_df = pd.read_csv(sonata_t)\n",
    "soul_df = pd.read_csv(soul_t)\n",
    "spark_df = pd.read_csv(spark_t)\n",
    "\n",
    "dfs = [sonata_df, soul_df, spark_df]\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_acc_wb, models_f1_wb, attacks_acc_wb, attacks_f1_wb, accuracy_wb, f1_wb = attacks_evaluation(df, bw='white')\n",
    "models_acc_gb, models_f1_gb, attacks_acc_gb, attacks_f1_gb, accuracy_gb, f1_gb = attacks_evaluation(df, bw='gray')\n",
    "models_acc_bb, models_f1_bb, attacks_acc_bb, attacks_f1_bb, accuracy_bb, f1_bb = attacks_evaluation(df, bw='black')\n",
    "\n",
    "print('[MODELS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    print(f'[{model_name}]\\t\\t{models_f1_wb[i]:.3f}\\t{models_f1_gb[i]:.3f}\\t{models_f1_bb[i]:.3f}')\n",
    "print()\n",
    "print('[ATTACKS F1]\\t[WHITE]\\t[GRAY]\\t[BLACK]')\n",
    "for i, attack_name in enumerate(attack_names):\n",
    "    print(f'[{attack_name}]\\t\\t{attacks_f1_wb[i]:.3f}\\t{attacks_f1_gb[i]:.3f}\\t{attacks_f1_bb[i]:.3f}')\n",
    "print()\n",
    "for vehicle in vehicles:\n",
    "    paths = [f'./models/{vehicle}/adversarial-training/all-models/FCN_online.pth', f'./models/{vehicle}/adversarial-training/all-models/CNN_online.pth', f'./models/{vehicle}/adversarial-training/all-models/LSTM_online.pth']\n",
    "    evaluate_baseline(vehicle, paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
